{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "class MultiDimensionalStochasticProgrammingEnv(gym.Env):\n",
    "    np.random.seed(0)\n",
    "    def __init__(self, num_products=3):\n",
    "        super(MultiDimensionalStochasticProgrammingEnv, self).__init__()\n",
    "        \n",
    "        self.num_products = num_products\n",
    "        \n",
    "        # 动作空间：每个产品的补货数量（0到10）\n",
    "        self.action_space = spaces.Box(low=0, high=300, shape=(num_products,), dtype=np.float32)\n",
    "        \n",
    "        # 状态空间：每个产品的当前库存数量\n",
    "        self.observation_space = spaces.Box(low=0.0, high=300.0, shape=(num_products,), dtype=np.float32)\n",
    "        self.sequence_t = 10\n",
    "        self.max_inventory = 300  # 每个产品的最大库存\n",
    "        self.current_inventory = np.full(num_products, 10)  # 初始库存\n",
    "        self.profit_per_unit = 10  # 每个产品的利润\n",
    "        self.store_cost_per_unit = 5  # 每个产品的储存成本\n",
    "        self.day = 0  # 天数计数器\n",
    "        self.max_days = 100  # 设定最大天数\n",
    "        self.weight_list = list(np.random.dirichlet(np.ones(int(self.max_days/self.sequence_t)), size=1).reshape(-1, 1).flatten())\n",
    "        \n",
    "    def reset(self):\n",
    "        self.current_inventory = np.full(self.num_products, 10)  # 重置每个产品的库存\n",
    "        self.total_reward = 0\n",
    "        self.day = 0  # 每个 episode 重置天数计数器\n",
    "        return self.current_inventory\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.current_inventory = np.minimum(self.max_inventory, self.current_inventory + action)\n",
    "\n",
    "        self.day += 1  # 计数天数增加\n",
    "        \n",
    "        # 随机生成每个产品的需求\n",
    "        demand = np.random.normal(loc=150, scale=1, size=self.num_products)\n",
    "        \n",
    "        # 实际满足的需求\n",
    "        satisfied_demand = np.minimum(self.current_inventory, demand)\n",
    "        self.current_inventory = self.current_inventory.astype(np.float64)  # 确保类型为 float64\n",
    "        self.current_inventory -= satisfied_demand\n",
    "        \n",
    "        reward = (np.sum(satisfied_demand * self.profit_per_unit) - np.sum(self.current_inventory * self.store_cost_per_unit))*self.weight_list[int(self.day/self.sequence_t)]\n",
    "        self.total_reward += reward\n",
    "        \n",
    "        # 当天数达到最大天数时，结束 episode\n",
    "        done = self.day >= self.max_days-1\n",
    "        \n",
    "        return self.current_inventory, reward, done, {}\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        print(f\"Current Inventory: {self.current_inventory}, Total Reward: {self.total_reward}, Day: {self.day}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.13910977875943475,\n",
       " 0.06673757821703034,\n",
       " 0.0744302408670181,\n",
       " 0.2303801706851375,\n",
       " 0.006533481190117616,\n",
       " 0.008083001971357874,\n",
       " 0.0018110815447159095,\n",
       " 0.15849169955337453,\n",
       " 0.13351381650944208,\n",
       " 0.1809091507023712]"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(1,10).reshape(-1,1)\n",
    "random_numbers = list(np.random.dirichlet(np.ones(10), size=1).reshape(-1, 1).flatten())\n",
    "random_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import rl_utils\n",
    "\n",
    "\n",
    "class PolicyNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return F.softmax(self.fc2(x), dim=1)\n",
    "\n",
    "\n",
    "class ValueNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim):\n",
    "        super(ValueNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PolicyNetContinuous(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(PolicyNetContinuous, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc_mu = torch.nn.Linear(hidden_dim, action_dim)\n",
    "        self.fc_std = torch.nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # 强制 mu 为正：使用 softplus 或 ReLU 激活函数\n",
    "        mu = F.softplus(self.fc_mu(x))  # softplus 是一个平滑的 ReLU 函数，输出正值\n",
    "        std = F.softplus(self.fc_std(x))  # 确保标准差为正\n",
    "        return mu, std\n",
    "\n",
    "\n",
    "\n",
    "class PPOContinuous:\n",
    "    ''' 处理连续动作的PPO算法 '''\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, actor_lr, critic_lr,\n",
    "                 lmbda, epochs, eps, gamma, device):\n",
    "        self.actor = PolicyNetContinuous(state_dim, hidden_dim,\n",
    "                                         action_dim).to(device)\n",
    "        self.critic = ValueNet(state_dim, hidden_dim).to(device)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),\n",
    "                                                lr=actor_lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),\n",
    "                                                 lr=critic_lr)\n",
    "        self.gamma = gamma\n",
    "        self.lmbda = lmbda\n",
    "        self.epochs = epochs\n",
    "        self.eps = eps\n",
    "        self.device = device\n",
    "\n",
    "    def take_action(self, state):\n",
    "        # Ensure that state is a tensor and move to the correct device\n",
    "        state = torch.tensor([state], dtype=torch.float).to(self.device)\n",
    "        \n",
    "        # Get the mean (mu) and standard deviation (sigma) from the actor network\n",
    "        mu, sigma = self.actor(state)\n",
    "        \n",
    "        # Ensure the output is of shape (batch_size, action_dim) which should be (1, 10)\n",
    "        action_dist = torch.distributions.Normal(mu, sigma)\n",
    "        \n",
    "        # Sample one action from the distribution\n",
    "        action = action_dist.sample()  # This will be of shape (1, action_dim) = (1, 10)\n",
    "        # Convert the action tensor to a 1D array (of size action_dim)\n",
    "        action = action.squeeze(0)  # Remove the batch dimension, resulting in a 10-dimensional tensor\n",
    "        \n",
    "        # Convert the action to a list and return it\n",
    "        return action.tolist()\n",
    "\n",
    "\n",
    "    def update(self, transition_dict):\n",
    "        states = torch.tensor(transition_dict['states'],\n",
    "                              dtype=torch.float).to(self.device)\n",
    "        actions = torch.tensor(transition_dict['actions'],\n",
    "                               dtype=torch.float).to(self.device)\n",
    "        rewards = torch.tensor(transition_dict['rewards'],\n",
    "                               dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        next_states = torch.tensor(transition_dict['next_states'],\n",
    "                                   dtype=torch.float).to(self.device)\n",
    "        dones = torch.tensor(transition_dict['dones'],\n",
    "                             dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        td_target = rewards + self.gamma * self.critic(next_states) * (1 -\n",
    "                                                                       dones)\n",
    "        td_delta = td_target - self.critic(states)\n",
    "        advantage = rl_utils.compute_advantage(self.gamma, self.lmbda,\n",
    "                                               td_delta.cpu()).to(self.device)\n",
    "        mu, std = self.actor(states)\n",
    "        action_dists = torch.distributions.Normal(mu.detach(), std.detach())\n",
    "        # 动作是正态分布\n",
    "\n",
    "        old_log_probs = action_dists.log_prob(actions)\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            mu, std = self.actor(states)\n",
    "            std = torch.clamp(std, min=1e-6)\n",
    "            action_dists = torch.distributions.Normal(mu, std)\n",
    "            log_probs = action_dists.log_prob(actions)\n",
    "            ratio = torch.exp(log_probs - old_log_probs)\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1 - self.eps, 1 + self.eps) * advantage\n",
    "            actor_loss = torch.mean(-torch.min(surr1, surr2))\n",
    "            critic_loss = torch.mean(\n",
    "                F.mse_loss(self.critic(states), td_target.detach()))\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            critic_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "            self.critic_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Backbone(nn.Module):\n",
    "    def __init__(self, time_steps, state_dim = 10,action_dim = 10, hidden_dim = 64):\n",
    "        super().__init__()\n",
    "        self.time_steps = time_steps\n",
    "        self.denoise_net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(state_dim + action_dim + 1, hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, state, noisy_action, time_step):\n",
    "        # 将时间步与 state 和 noisy_action 结合\n",
    "        time_embedding = time_step.view(-1, 1).float() / self.time_steps\n",
    "        input_data = torch.cat([state, noisy_action, time_embedding], dim=1)\n",
    "        # 输出去噪后的动作\n",
    "        return self.denoise_net(input_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, device, beta_1, beta_T, T, state_dim = 10,action_dim = 10):\n",
    "        '''\n",
    "        The epsilon predictor of diffusion process.\n",
    "\n",
    "        beta_1    : beta_1 of diffusion process\n",
    "        beta_T    : beta_T of diffusion process\n",
    "        T         : Diffusion Steps\n",
    "        input_dim : a dimension of data\n",
    "\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.alpha_bars = torch.cumprod(1 - torch.linspace(start = beta_1, end=beta_T, steps=T), dim = 0).to(device = device)\n",
    "        self.backbone = Backbone(T, state_dim, action_dim)\n",
    "        \n",
    "        self.to(device = self.device)\n",
    "\n",
    "    def loss_fn(self, x,state, idx=None):\n",
    "        '''\n",
    "        This function performed when only training phase.\n",
    "\n",
    "        x          : real data if idx==None else perturbation data\n",
    "        idx        : if None (training phase), we perturbed random index. Else (inference phase), it is recommended that you specify.\n",
    "\n",
    "        '''\n",
    "        output, epsilon, alpha_bar = self.forward(x,state, idx=idx, get_target=True)\n",
    "        loss = (output - epsilon).square().mean()\n",
    "        return loss\n",
    "\n",
    "        \n",
    "    def forward(self, x,state, idx=None, get_target=False):\n",
    "        '''\n",
    "        x          : real data if idx==None else perturbation data\n",
    "        idx        : if None (training phase), we perturbed random index. Else (inference phase), it is recommended that you specify.\n",
    "        get_target : if True (training phase), target and sigma is returned with output (epsilon prediction)\n",
    "\n",
    "        '''\n",
    "\n",
    "        if idx == None:\n",
    "            idx = torch.randint(0, len(self.alpha_bars), (x.size(0), )).to(device = self.device)\n",
    "            used_alpha_bars = self.alpha_bars[idx][:, None]\n",
    "            epsilon = torch.randn_like(x)\n",
    "            x_tilde = torch.sqrt(used_alpha_bars) * x + torch.sqrt(1 - used_alpha_bars) * epsilon\n",
    "            \n",
    "        else:\n",
    "            idx = torch.Tensor([idx for _ in range(x.size(0))]).to(device = self.device).long()\n",
    "            x_tilde = x\n",
    "            \n",
    "\n",
    "        \n",
    "        output = self.backbone(x_tilde,state, idx)\n",
    "        \n",
    "        return (output, epsilon, used_alpha_bars) if get_target else output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionProcess():\n",
    "    def __init__(self, beta_1, beta_T, T, diffusion_fn, device, datadim):\n",
    "        '''\n",
    "        beta_1        : beta_1 of diffusion process\n",
    "        beta_T        : beta_T of diffusion process\n",
    "        T             : step of diffusion process\n",
    "        diffusion_fn  : trained diffusion network\n",
    "        datadim         : data dimension\n",
    "        '''\n",
    "\n",
    "        self.betas = torch.linspace(start = beta_1, end=beta_T, steps=T)\n",
    "        self.alphas = 1 - self.betas\n",
    "        self.alpha_bars = torch.cumprod(1 - torch.linspace(start = beta_1, end=beta_T, steps=T), dim = 0).to(device = device)\n",
    "        self.alpha_prev_bars = torch.cat([torch.Tensor([1]).to(device=device), self.alpha_bars[:-1]])\n",
    "        self.datadim = datadim\n",
    "        \n",
    "        self.diffusion_fn = diffusion_fn\n",
    "        self.device = device\n",
    "\n",
    "    \n",
    "    def _one_diffusion_step(self, x):\n",
    "        '''\n",
    "        x   : perturbated data\n",
    "        '''\n",
    "        for idx in reversed(range(len(self.alpha_bars))):\n",
    "            noise = torch.zeros_like(x) if idx == 0 else torch.randn_like(x)\n",
    "            sqrt_tilde_beta = torch.sqrt((1 - self.alpha_prev_bars[idx]) / (1 - self.alpha_bars[idx]) * self.betas[idx])\n",
    "            predict_epsilon = self.diffusion_fn(x, idx)\n",
    "            mu_theta_xt = torch.sqrt(1 / self.alphas[idx]) * (x - self.betas[idx] / torch.sqrt(1 - self.alpha_bars[idx]) * predict_epsilon)\n",
    "            x = mu_theta_xt + sqrt_tilde_beta * noise\n",
    "            yield x\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sampling(self, sampling_number, only_final=False):\n",
    "        '''\n",
    "        sampling_number : a number of generation\n",
    "        only_final      : If True, return is an only output of final schedule step \n",
    "        '''\n",
    "        sample = torch.randn([sampling_number,self.datadim]).to(device = self.device).squeeze()\n",
    "        sampling_list = []\n",
    "        \n",
    "        final = None\n",
    "        for idx, sample in enumerate(self._one_diffusion_step(sample)):\n",
    "            final = sample\n",
    "            if not only_final:\n",
    "                sampling_list.append(final)\n",
    "\n",
    "        return final if only_final else torch.stack(sampling_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionProcess():\n",
    "    def __init__(self, beta_1, beta_T, T, diffusion_fn, device, datadim):\n",
    "        '''\n",
    "        beta_1        : beta_1 of diffusion process\n",
    "        beta_T        : beta_T of diffusion process\n",
    "        T             : step of diffusion process\n",
    "        diffusion_fn  : trained diffusion network\n",
    "        datadim         : data dimension\n",
    "        '''\n",
    "\n",
    "        self.betas = torch.linspace(start = beta_1, end=beta_T, steps=T)\n",
    "        self.alphas = 1 - self.betas\n",
    "        self.alpha_bars = torch.cumprod(1 - torch.linspace(start = beta_1, end=beta_T, steps=T), dim = 0).to(device = device)\n",
    "        self.alpha_prev_bars = torch.cat([torch.Tensor([1]).to(device=device), self.alpha_bars[:-1]])\n",
    "        self.datadim = datadim\n",
    "        \n",
    "        self.diffusion_fn = diffusion_fn\n",
    "        self.device = device\n",
    "\n",
    "    \n",
    "    def _one_diffusion_step(self, x,state, idx=None):\n",
    "        '''\n",
    "        x   : perturbated data\n",
    "        '''\n",
    "        for idx in reversed(range(len(self.alpha_bars))):\n",
    "            noise = torch.zeros_like(x) if idx == 0 else torch.randn_like(x)\n",
    "            sqrt_tilde_beta = torch.sqrt((1 - self.alpha_prev_bars[idx]) / (1 - self.alpha_bars[idx]) * self.betas[idx])\n",
    "            predict_epsilon = self.diffusion_fn(x, state,idx)\n",
    "            mu_theta_xt = torch.sqrt(1 / self.alphas[idx]) * (x - self.betas[idx] / torch.sqrt(1 - self.alpha_bars[idx]) * predict_epsilon)\n",
    "            x = mu_theta_xt + sqrt_tilde_beta * noise\n",
    "            yield x\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sampling(self, state, sampling_number, only_final=False):\n",
    "        '''\n",
    "        sampling_number : a number of generation\n",
    "        only_final      : If True, return is an only output of final schedule step \n",
    "        '''\n",
    "        sample = torch.randn([sampling_number,self.datadim]).to(device = self.device).squeeze()\n",
    "        sampling_list = []\n",
    "        \n",
    "        final = None\n",
    "        for idx, sample in enumerate(self._one_diffusion_step(sample,state)):\n",
    "            final = sample\n",
    "            if not only_final:\n",
    "                sampling_list.append(final)\n",
    "\n",
    "        return final if only_final else torch.stack(sampling_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ddpm(model, optimizer, num_epochs, state_loader, action_loader,state_loader_v, action_loader_v, early_stopping):\n",
    "    best_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        whole_loss = 0\n",
    "        for i, (state_batch, action_batch) in enumerate(zip(state_loader, action_loader)):\n",
    "            state_batch = state_batch.cuda()\n",
    "            action_batch = action_batch.cuda()\n",
    "            loss = model.loss_fn(action_batch, state_batch)\n",
    "            whole_loss += loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for state_val_batch, action_val_batch in zip(state_loader_v, action_loader_v):\n",
    "                state_val_batch = state_val_batch.cuda()\n",
    "                action_val_batch = action_val_batch.cuda()\n",
    "                val_loss += model.loss_fn(action_val_batch, state_val_batch)\n",
    "            val_loss /= len(state_loader)\n",
    "        if (epoch) % 20 == 0:\n",
    "            print('epoch: {}, Train Loss: {:.4f}, Val Loss: {:.4f}'.format(epoch, whole_loss/len(state_loader), val_loss.item()))\n",
    "        loss_new = val_loss\n",
    "        if loss_new < best_loss:\n",
    "            best_loss = loss_new\n",
    "            early_stopping_counter = 0\n",
    "            print('epoch: {}, find new best loss: Train Loss: {:.4f}'.format(epoch, best_loss))\n",
    "            print('-' * 10)\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "        if early_stopping_counter == early_stopping:\n",
    "            print(\"Early stopping after {} epochs\".format(epoch))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class PolicyNetContinuous(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(PolicyNetContinuous, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc_mu = torch.nn.Linear(hidden_dim, action_dim)\n",
    "        self.fc_std = torch.nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        mu = self.fc_mu(x)\n",
    "        std = F.softplus(self.fc_std(x)) + 1e-3  # 确保标准差正值\n",
    "        return mu, std\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import collections\n",
    "import random\n",
    "env = MultiDimensionalStochasticProgrammingEnv(num_products=10)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "action_bound = env.action_space.high\n",
    "actor_lr = 1e-4\n",
    "critic_lr = 5e-3\n",
    "num_episodes = 1000\n",
    "hidden_dim = 128\n",
    "gamma = 0.9\n",
    "lmbda = 0.9\n",
    "epochs = 10\n",
    "eps = 0.2\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\n",
    "    \"cpu\")\n",
    "agent = PPOContinuous(state_dim, hidden_dim, action_dim, actor_lr, critic_lr,\n",
    "                      lmbda, epochs, eps, gamma, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train_on_policy_agent(env, agent, num_episodes):\n",
    "    return_list = []\n",
    "    transition_dict_whole = {'states': [], 'actions': [], 'next_states': [], 'rewards': [], 'dones': []}\n",
    "    for i in range(5):\n",
    "        with tqdm(total=int(num_episodes/10), desc='Iteration %d' % i) as pbar:\n",
    "            for i_episode in range(int(num_episodes/10)):\n",
    "                episode_return = 0\n",
    "                transition_dict = {'states': [], 'actions': [], 'next_states': [], 'rewards': [], 'dones': []}\n",
    "                state = env.reset()\n",
    "                done = False\n",
    "                while not done:\n",
    "                    action = agent.take_action(state)\n",
    "                    next_state, reward, done, _ = env.step(action)\n",
    "                    transition_dict['states'].append(state)\n",
    "                    transition_dict['actions'].append(action)\n",
    "                    transition_dict['next_states'].append(next_state)\n",
    "                    transition_dict['rewards'].append(reward)\n",
    "                    transition_dict['dones'].append(done)\n",
    "                    transition_dict_whole['states'].append(state)\n",
    "                    transition_dict_whole['actions'].append(action)\n",
    "                    transition_dict_whole['next_states'].append(next_state)\n",
    "                    transition_dict_whole['rewards'].append(reward)\n",
    "                    transition_dict_whole['dones'].append(done)\n",
    "                    state = next_state\n",
    "                    episode_return += reward\n",
    "                return_list.append(episode_return)\n",
    "                agent.update(transition_dict)\n",
    "                if (i_episode+1) % 10 == 0:\n",
    "                    pbar.set_postfix({'episode': '%d' % (num_episodes/10 * i + i_episode+1), 'return': '%.3f' % np.mean(return_list[-10:])})\n",
    "                pbar.update(1)\n",
    "    return return_list,transition_dict_whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 0: 100%|██████████| 100/100 [00:08<00:00, 12.42it/s, episode=100, return=926.634]\n",
      "Iteration 1: 100%|██████████| 100/100 [00:07<00:00, 12.79it/s, episode=200, return=1230.247]\n",
      "Iteration 2: 100%|██████████| 100/100 [00:08<00:00, 12.01it/s, episode=300, return=1641.979]\n",
      "Iteration 3: 100%|██████████| 100/100 [00:08<00:00, 11.53it/s, episode=400, return=1941.155]\n",
      "Iteration 4: 100%|██████████| 100/100 [00:08<00:00, 12.39it/s, episode=500, return=2344.127]\n"
     ]
    }
   ],
   "source": [
    "return_list,transition_dict = train_on_policy_agent(env, agent, num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_dict['states'] = np.array(transition_dict['states'])\n",
    "transition_dict['actions'] = np.array(transition_dict['actions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([49500, 10])"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试组合模型\n",
    "state_dim = 10\n",
    "hidden_dim = 64\n",
    "action_dim = 10\n",
    "time_steps = 1000\n",
    "batch_size = 32\n",
    "\n",
    "transition_dict['actions'] = np.array(transition_dict['actions'])\n",
    "transition_dict['actions'].shape\n",
    "state = torch.tensor(transition_dict['states'], dtype=torch.float).to(device)\n",
    "action = torch.tensor(transition_dict['actions'], dtype=torch.float).to(device)\n",
    "batch_size =64\n",
    "beta_1 = 1e-4\n",
    "beta_T = 0.02\n",
    "T = 50\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\n",
    "    \"cpu\")\n",
    "model = Model(device, beta_1, beta_T, T)\n",
    "optim = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "model(state,action, idx=0, get_target=False).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert tensors to numpy arrays\n",
    "state_np = state.detach().cpu().numpy()\n",
    "action_np = action.detach().cpu().numpy()\n",
    "sc = StandardScaler()\n",
    "action_np = sc.fit_transform(action_np)\n",
    "\n",
    "# Define the sizes for training, validation, and test sets\n",
    "train_size = 5000\n",
    "val_size = 5000\n",
    "test_size = 1000\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "state_train, state_temp, action_train, action_temp = train_test_split(state_np, action_np, train_size=train_size, random_state=42)\n",
    "state_val, state_test, action_val, action_test = train_test_split(state_temp, action_temp, test_size=test_size, random_state=42)\n",
    "\n",
    "# Convert the numpy arrays to DataLoader\n",
    "state_train = DataLoader(state_train, batch_size=batch_size, shuffle=True)\n",
    "state_val = DataLoader(state_val, batch_size=batch_size, shuffle=True)\n",
    "state_test = DataLoader(state_test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "action_train = DataLoader(action_train, batch_size=batch_size, shuffle=True)\n",
    "action_val = DataLoader(action_val, batch_size=batch_size, shuffle=True)\n",
    "action_test = DataLoader(action_test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, Train Loss: 0.9839, Val Loss: 7.9200\n",
      "epoch: 0, find new best loss: Train Loss: 7.9200\n",
      "----------\n",
      "epoch: 1, find new best loss: Train Loss: 7.4123\n",
      "----------\n",
      "epoch: 2, find new best loss: Train Loss: 7.0940\n",
      "----------\n",
      "epoch: 3, find new best loss: Train Loss: 6.9726\n",
      "----------\n",
      "epoch: 4, find new best loss: Train Loss: 6.9240\n",
      "----------\n",
      "epoch: 5, find new best loss: Train Loss: 6.8979\n",
      "----------\n",
      "epoch: 6, find new best loss: Train Loss: 6.8527\n",
      "----------\n",
      "epoch: 7, find new best loss: Train Loss: 6.8258\n",
      "----------\n",
      "epoch: 8, find new best loss: Train Loss: 6.8074\n",
      "----------\n",
      "epoch: 10, find new best loss: Train Loss: 6.8026\n",
      "----------\n",
      "epoch: 11, find new best loss: Train Loss: 6.8008\n",
      "----------\n",
      "epoch: 12, find new best loss: Train Loss: 6.7706\n",
      "----------\n",
      "epoch: 13, find new best loss: Train Loss: 6.7662\n",
      "----------\n",
      "epoch: 14, find new best loss: Train Loss: 6.7358\n",
      "----------\n",
      "epoch: 19, find new best loss: Train Loss: 6.7016\n",
      "----------\n",
      "epoch: 20, Train Loss: 0.7833, Val Loss: 6.7139\n",
      "epoch: 23, find new best loss: Train Loss: 6.6991\n",
      "----------\n",
      "epoch: 25, find new best loss: Train Loss: 6.6987\n",
      "----------\n",
      "epoch: 26, find new best loss: Train Loss: 6.6977\n",
      "----------\n",
      "epoch: 27, find new best loss: Train Loss: 6.6803\n",
      "----------\n",
      "epoch: 29, find new best loss: Train Loss: 6.6669\n",
      "----------\n",
      "epoch: 31, find new best loss: Train Loss: 6.6466\n",
      "----------\n",
      "epoch: 32, find new best loss: Train Loss: 6.6249\n",
      "----------\n",
      "epoch: 40, Train Loss: 0.7690, Val Loss: 6.6272\n",
      "epoch: 41, find new best loss: Train Loss: 6.6226\n",
      "----------\n",
      "epoch: 42, find new best loss: Train Loss: 6.6192\n",
      "----------\n",
      "epoch: 43, find new best loss: Train Loss: 6.6168\n",
      "----------\n",
      "epoch: 45, find new best loss: Train Loss: 6.5967\n",
      "----------\n",
      "epoch: 49, find new best loss: Train Loss: 6.5937\n",
      "----------\n",
      "epoch: 52, find new best loss: Train Loss: 6.5877\n",
      "----------\n",
      "epoch: 56, find new best loss: Train Loss: 6.5621\n",
      "----------\n",
      "epoch: 60, Train Loss: 0.7680, Val Loss: 6.6078\n",
      "epoch: 65, find new best loss: Train Loss: 6.5562\n",
      "----------\n",
      "epoch: 73, find new best loss: Train Loss: 6.5539\n",
      "----------\n",
      "epoch: 78, find new best loss: Train Loss: 6.5536\n",
      "----------\n",
      "epoch: 80, Train Loss: 0.7702, Val Loss: 6.5551\n",
      "epoch: 85, find new best loss: Train Loss: 6.5488\n",
      "----------\n",
      "epoch: 87, find new best loss: Train Loss: 6.5486\n",
      "----------\n",
      "epoch: 89, find new best loss: Train Loss: 6.5476\n",
      "----------\n",
      "epoch: 92, find new best loss: Train Loss: 6.5371\n",
      "----------\n",
      "epoch: 100, Train Loss: 0.7598, Val Loss: 6.5215\n",
      "epoch: 100, find new best loss: Train Loss: 6.5215\n",
      "----------\n",
      "epoch: 120, Train Loss: 0.7663, Val Loss: 6.5641\n",
      "epoch: 140, Train Loss: 0.7624, Val Loss: 6.5697\n",
      "epoch: 160, Train Loss: 0.7533, Val Loss: 6.5228\n",
      "epoch: 180, Train Loss: 0.7599, Val Loss: 6.5419\n",
      "epoch: 190, find new best loss: Train Loss: 6.5151\n",
      "----------\n",
      "epoch: 200, Train Loss: 0.7666, Val Loss: 6.5435\n",
      "epoch: 208, find new best loss: Train Loss: 6.5142\n",
      "----------\n",
      "epoch: 220, Train Loss: 0.7629, Val Loss: 6.5320\n",
      "epoch: 227, find new best loss: Train Loss: 6.4951\n",
      "----------\n",
      "epoch: 240, Train Loss: 0.7674, Val Loss: 6.5289\n",
      "epoch: 260, Train Loss: 0.7683, Val Loss: 6.5118\n",
      "epoch: 280, Train Loss: 0.7636, Val Loss: 6.5308\n",
      "epoch: 300, Train Loss: 0.7669, Val Loss: 6.5235\n",
      "epoch: 320, Train Loss: 0.7547, Val Loss: 6.5013\n",
      "Early stopping after 327 epochs\n"
     ]
    }
   ],
   "source": [
    "train_ddpm(model, optim, 500, state_train, action_train,state_val, action_val, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_train, state_temp, action_train, action_temp = train_test_split(state_np, action_np, train_size=train_size, random_state=42)\n",
    "state_val, state_test, action_val, action_test = train_test_split(state_temp, action_temp, test_size=test_size, random_state=42)\n",
    "sampling_number = state_test.shape[0]\n",
    "only_final = True\n",
    "process = DiffusionProcess(beta_1, beta_T, T, model, device, action_dim)\n",
    "action = process.sampling(torch.tensor(state_test).to(device=device),sampling_number, only_final)\n",
    "action = sc.inverse_transform(action.cpu().numpy())\n",
    "torch.save(model.state_dict(), 'ddpm_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 3.0509133\n",
      "[ 0.1845298   1.4341173   1.299663   -0.07048453  0.70084965 -0.24861439\n",
      "  1.3923907   0.9426981   1.1415417   0.22510754]\n"
     ]
    }
   ],
   "source": [
    "print('Mean Squared Error:', np.mean((action_test - action)**2))\n",
    "print(action[4])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
